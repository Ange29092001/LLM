{"cells":[{"cell_type":"markdown","metadata":{"id":"lCJvlnvsKALE"},"source":["<center>\n","<h1></h1>\n","<h1>Deep Learning for Text and LLMs</h1>\n","<h2>Lab Session 2: Neural Machine Translation</h2>\n","<h5>February 6, 2024</h5>\n","<h4><b>Student Name:</b> </h4>\n","<br>\n","</center>\n","\n","<hr style=\"border:5px solid gray\"> </hr>"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"DB6pvLvlKbtD","executionInfo":{"status":"ok","timestamp":1707235317533,"user_tz":-60,"elapsed":5007,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils import data\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","from nltk import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"wqIFlSfYTwk8"},"source":["## Define the Encoder / Task 1"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Kc8cQTFkKmif","executionInfo":{"status":"ok","timestamp":1707235317534,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["class Encoder(nn.Module):\n","    '''\n","    to be passed the entire source sequence at once\n","    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n","    https://pytorch.org/docs/stable/nn.html#gru\n","    '''\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n","        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n","\n","    def forward(self, input):\n","        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n","        # you should return a tensor of shape (seq, batch, feat)\n","        embedded = self.embedding(input)  # input shape: (seq_len, batch_size)\n","        hs, hidden = self.rnn(embedded)\n","        return hs"]},{"cell_type":"markdown","metadata":{"id":"FNnGEa5cT9ka"},"source":["## Define the Decoder layer / Task 2"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"h7tLaq4PK90q","executionInfo":{"status":"ok","timestamp":1707235320177,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","    '''to be used one timestep at a time\n","       see https://pytorch.org/docs/stable/nn.html#gru'''\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n","        super(Decoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n","        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n","        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n","        self.predict = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, input, source_context, h):\n","        # fill the gaps #\n","        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n","        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n","        embedded = self.embedding(input)  # input shape: (1, batch_size)\n","        h, _ = self.rnn(embedded, h)\n","        h_tilde = torch.tanh(self.ff_concat(torch.cat((source_context, h), -1)))\n","        prediction = self.predict(h_tilde)\n","        return prediction, h"]},{"cell_type":"markdown","metadata":{"id":"bn9iO9wNT2p7"},"source":["## Define the Attention layer / Task 3"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"JwUAUDL4KmoM","executionInfo":{"status":"ok","timestamp":1707235323085,"user_tz":-60,"elapsed":327,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["class seq2seqAtt(nn.Module):\n","    '''\n","    concat global attention a la Luong et al. 2015 (subsection 3.1)\n","    https://arxiv.org/pdf/1508.04025.pdf\n","    '''\n","    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n","        super(seq2seqAtt, self).__init__()\n","        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n","        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n","\n","    def forward(self, target_h, source_hs):\n","        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n","        # fill the gaps #\n","        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n","        concat_output = self.ff_concat(torch.cat((target_h_rep, source_hs), -1))\n","        scores = self.ff_score(torch.tanh(concat_output)) # should be of shape (seq, batch, 1)\n","        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n","        norm_scores = torch.softmax(scores, 0)\n","        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n","        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n","        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes\n","        return ct"]},{"cell_type":"markdown","metadata":{"id":"VUT6D3JETX8H"},"source":["# Define the full seq2seq model / Task 4:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"FYX0K3dNK-c9","executionInfo":{"status":"ok","timestamp":1707235326500,"user_tz":-60,"elapsed":328,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["class seq2seqModel(nn.Module):\n","    '''the full seq2seq model'''\n","    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n","     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n","     'oov_token','sos_token','eos_token','max_size']\n","    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t,\n","                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n","                 oov_token, sos_token, eos_token, max_size):\n","        super(seq2seqModel, self).__init__()\n","        self.vocab_s = vocab_s\n","        self.source_language = source_language\n","        self.vocab_t_inv = vocab_t_inv\n","        self.embedding_dim_s = embedding_dim_s\n","        self.embedding_dim_t = embedding_dim_t\n","        self.hidden_dim_s = hidden_dim_s\n","        self.hidden_dim_t = hidden_dim_t\n","        self.hidden_dim_att = hidden_dim_att\n","        self.do_att = do_att # should attention be used?\n","        self.padding_token = padding_token\n","        self.oov_token = oov_token\n","        self.sos_token = sos_token\n","        self.eos_token = eos_token\n","        self.max_size = max_size\n","\n","        self.max_source_idx = max(list(vocab_s.values()))\n","        print('max source index',self.max_source_idx)\n","        print('source vocab size',len(vocab_s))\n","\n","        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n","        print('max target index',self.max_target_idx)\n","        print('target vocab size',len(vocab_t_inv))\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n","        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n","\n","        if self.do_att:\n","            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n","\n","    def my_pad(self, my_list):\n","        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n","        the <eos> token is appended to each sequence before padding\n","        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n","        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n","        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n","        return batch_source, batch_target\n","\n","    def forward(self, input, max_size, is_prod):\n","        if is_prod:\n","            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n","        current_batch_size = input.size(1)\n","        # fill the gap #\n","        # use the encoder\n","        source_hs = self.encoder(input)\n","        # = = = decoder part (one timestep at a time)  = = =\n","        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n","\n","        # fill the gap #\n","        # (initialize target_input with the proper token)\n","        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n","        pos = 0\n","        eos_counter = 0\n","        logits = []\n","\n","        while True:\n","            if self.do_att:\n","                source_context = self.att_mech(target_h, source_hs) # (1, batch, feat)\n","            else:\n","                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n","            # fill the gap #\n","            # use the decoder\n","            prediction, target_h = self.decoder(target_input, source_context, target_h)\n","            logits.append(prediction) # (1, batch, vocab)\n","            # fill the gap #\n","            # get the next input to pass the decoder\n","            target_input = prediction.argmax(-1) # the predicted word\n","            eos_counter += torch.sum(target_input==self.eos_token).item()\n","            pos += 1\n","            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n","                break\n","        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n","\n","        if is_prod:\n","            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n","\n","        return to_return\n","\n","    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n","        parameters = [p for p in self.parameters() if p.requires_grad]\n","        optimizer = optim.Adam(parameters, lr=lr)\n","        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n","        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n","        # we pass a collate function to perform padding on the fly, within each batch\n","        # this is better than truncation/padding at the dataset level\n","        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size,\n","                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n","        test_loader = data.DataLoader(testDataset, batch_size=512,\n","                                      collate_fn=self.my_pad)\n","        tdqm_dict_keys = ['loss', 'test loss']\n","        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n","        patience_counter = 1\n","        patience_loss = 99999\n","\n","        for epoch in range(n_epochs):\n","            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n","                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n","                for loader_idx, loader in enumerate([train_loader, test_loader]):\n","                    total_loss = 0\n","                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","                    if loader_idx == 0:\n","                        self.train()\n","                    else:\n","                        self.eval()\n","                    for i, (batch_source, batch_target) in enumerate(loader):\n","                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)\n","                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n","\n","                        # are we using the model in production\n","                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n","                        if is_prod:\n","                            max_size = self.max_size\n","                            self.eval()\n","                        else:\n","                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n","\n","                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n","                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n","                        total_loss += sentence_loss.item()\n","                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n","                        pbar.set_postfix(tdqm_dict)\n","                        if loader_idx == 0:\n","                            optimizer.zero_grad() # flush gradient attributes\n","                            sentence_loss.backward() # compute gradients\n","                            optimizer.step() # update\n","                            pbar.update(1)\n","\n","            if total_loss > patience_loss:\n","                patience_counter += 1\n","            else:\n","                patience_loss = total_loss\n","                patience_counter = 1 # reset\n","\n","            if patience_counter > patience:\n","                break\n","\n","    def sourceNl_to_ints(self, source_nl):\n","        '''converts natural language source sentence into source integers'''\n","        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n","        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n","        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n","                       self.oov_token for elt in source_nl_clean_tok]\n","\n","        source_ints = torch.LongTensor(source_ints).to(self.device)\n","        return source_ints\n","\n","    def targetInts_to_nl(self, target_ints):\n","        '''converts integer target sentence into target natural language'''\n","        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n","                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n","                else self.vocab_t_inv[elt] for elt in target_ints]\n","\n","    def predict(self, source_nl):\n","        source_ints = self.sourceNl_to_ints(source_nl)\n","        logits = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n","        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n","        target_nl = self.targetInts_to_nl(target_ints.tolist())\n","        return ' '.join(target_nl)\n","\n","    def save(self, path_to_file):\n","        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n","        attrs['state_dict'] = self.state_dict()\n","        torch.save(attrs, path_to_file)\n","\n","    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n","    def load(cls, path_to_file):\n","        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n","        state_dict = attrs.pop('state_dict')\n","        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n","        new.load_state_dict(state_dict)\n","        return new"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"B5RprtnBK-ia","executionInfo":{"status":"ok","timestamp":1707235340597,"user_tz":-60,"elapsed":306,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["import sys\n","import json\n","import urllib\n","\n","import torch\n","from torch.utils import data"]},{"cell_type":"markdown","metadata":{"id":"PgkVw6lVUIT3"},"source":["## Prepare the Data:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354197,"status":"ok","timestamp":1707222435518,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"},"user_tz":-60},"id":"datl5SFtJ9Br","outputId":"baa42ea7-beaf-416b-9b1a-b9f653aa76ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  data.zip\n","replace pairs_test_ints.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"," extracting: pairs_test_ints.txt     \n"," extracting: pairs_train_ints.txt    \n"," extracting: README.txt              \n"," extracting: vocab_source.json       \n"," extracting: vocab_target.json       \n"]}],"source":["urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\", 'data.zip')\n","urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\", 'pretrained_moodle.pt')\n","!unzip data.zip\n","\n","path_to_data = './'\n","path_to_save_models = './'"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"wZCiFl61LPQj","executionInfo":{"status":"ok","timestamp":1707235344990,"user_tz":-60,"elapsed":361,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["class Dataset(data.Dataset):\n","  def __init__(self, pairs):\n","        self.pairs = pairs\n","\n","  def __len__(self):\n","        return len(self.pairs) # total nb of observations\n","\n","  def __getitem__(self, idx):\n","        source, target = self.pairs[idx] # one observation\n","        return torch.LongTensor(source), torch.LongTensor(target)\n","\n","def load_pairs(train_or_test):\n","    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n","        pairs_tmp = file.read().splitlines()\n","    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n","    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n","                  elt[1].split()]] for elt in pairs_tmp]\n","    return pairs_tmp"]},{"cell_type":"markdown","metadata":{"id":"tCsAk4ILTkEc"},"source":["## Training / Task 5:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"kSZ-cvSuLQVt","outputId":"1cfd1a93-c340-426e-e01a-49af3df1b1b4","executionInfo":{"status":"error","timestamp":1707235365675,"user_tz":-60,"elapsed":288,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'path_to_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-b377f2078111>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_prod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpairs_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpairs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-09986f614887>\u001b[0m in \u001b[0;36mload_pairs\u001b[0;34m(train_or_test)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'pairs_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_or_test\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ints.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpairs_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpairs_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs_tmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'path_to_data' is not defined"]}],"source":["do_att = True # should always be set to True\n","is_prod = False # production mode or not\n","\n","if not is_prod:\n","\n","    pairs_train = load_pairs('train')\n","    pairs_test = load_pairs('test')\n","\n","    with open(path_to_data + 'vocab_source.json','r') as file:\n","        vocab_source = json.load(file) # word -> index\n","\n","    with open(path_to_data + 'vocab_target.json','r') as file:\n","        vocab_target = json.load(file) # word -> index\n","\n","    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n","\n","    print('data loaded')\n","\n","    training_set = Dataset(pairs_train)\n","    test_set = Dataset(pairs_test)\n","\n","    print('data prepared')\n","\n","    print('= = = attention-based model?:',str(do_att),'= = =')\n","\n","    model = seq2seqModel(vocab_s=vocab_source,\n","                         source_language='english',\n","                         vocab_t_inv=vocab_target_inv,\n","                         embedding_dim_s=40,\n","                         embedding_dim_t=40,\n","                         hidden_dim_s=30,\n","                         hidden_dim_t=30,\n","                         hidden_dim_att=20,\n","                         do_att=do_att,\n","                         padding_token=0,\n","                         oov_token=1,\n","                         sos_token=2,\n","                         eos_token=3,\n","                         max_size=30) # max size of generated sentence in prediction mode\n","\n","    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n","    model.save(path_to_save_models + 'my_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"pf0rN4RPToom"},"source":["## Testing / Task 6:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"UCvZmwWoCTUT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707235301014,"user_tz":-60,"elapsed":2303,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}},"outputId":"ad608a2d-bb5e-497d-9278-5c0051614bd0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"VhXbQjP_YrgY","executionInfo":{"status":"ok","timestamp":1707235378120,"user_tz":-60,"elapsed":298,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["Ais_prod = True # production mode or not\n","\n","if is_prod:\n","    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n","\n","    to_test = ['I am a student.',\n","               'I have a red car.',  # inversion captured\n","               'I love playing video games.',\n","                'This river is full of fish.', # plein vs pleine (accord)\n","                'The fridge is full of food.',\n","               'The cat fell asleep on the mat.',\n","               'my brother likes pizza.', # pizza is translated to 'la pizza'\n","               'I did not mean to hurt you', # translation of mean in context\n","               'She is so mean',\n","               'Help me pick out a tie to go with this suit!', # right translation\n","               \"I can't help but smoking weed\", # this one and below: hallucination\n","               'The kids were playing hide and seek',\n","               'The cat fell asleep in front of the fireplace']\n","\n","    for elt in to_test:\n","        print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44fKMCLsbNvr"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1U5-WLzs8uA8ryyrem6oggEKSwodnWwwx","timestamp":1707205258033},{"file_id":"13ZR8cUhxNIOyZs-SWTPNWtPf3fE1duTe","timestamp":1707163638694}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}