{"cells":[{"cell_type":"markdown","metadata":{"id":"fMIyjj8TskT_"},"source":["<center>\n","<h1></h1>\n","<h1>Deep Learning for Text and LLMs</h1>\n","<h2>Lab Session 1: Word Embeddings</h2>\n","<h5>February 5, 2024</h5>\n","<h4><b>Student Name:</b> </h4>\n","<br>\n","</center>\n","\n","<hr style=\"border:5px solid gray\"> </hr>\n","\n","\n","<h3><b>1. Introduction</b></h2>\n","<p style=\"text-align: justify;\">\n","Compared to traditional machine learning approaches (e.g., TF-IDF + SVM) that consider words and combinations of them as unique dimensions of the feature space, deep learning models embed words as vectors in a low-dimensional continuous space where dimensions represent shared latent concepts. The main advantages of this approach are (1) the ability to capture the similarity between words and thus to share predictive power between them; and (2) escaping the curse of dimensionality. Word embeddings can be initialized randomly and learned during training, or be pre-trained. In NLP, pre-trained word vectors obtained with word2vec from very large corpora are often used. The pre-trained word vectors can then be updated during training, or be kept static.\n","</p>\n","\n","<h3><b>2. Learning Objective</b></h2>\n","<p style=\"text-align: justify;\">\n","In this lab, you will learn about one of the two word2vec variants: skip-gram with negative sampling. We will derive and implement the model by hand using NumPy, and train it on a subset of the Internet Movie Database (IMDB) <a href='http://ai.stanford.edu/ Ìƒamaas/data/sentiment/'>dataset</a>.\n","Readings: the original paper introducing skip-gram (along with the other word2vec variant, CBOW) is [<a href='https://arxiv.org/abs/1301.3781'>Mikolov et al., 2013a</a>]. Negative sampling was proposed in a follow-up paper [<a href='https://arxiv.org/abs/1310.4546'>Mikolov et al., 2013b</a>]. Helpful resources to understand how skip-gram works are [<a href='https://arxiv.org/abs/1402.3722'>Goldberg et Levy, 2014</a>] and subsection 3.1 of [<a href='https://arxiv.org/abs/1607.04606'>Bojanowski et al., 2017</a>].\n","</p>\n","\n","<h3><b>3. Preprocessing</b></h2>\n","<p style=\"text-align: justify;\">\n","Before sampling training examples, the raw data need to be cleaned. Then, each document has to be converted into a list of integers. The integers correspond to indexes in a vocabulary (dictionary) in which the most frequent word has index 1 and only the words that appear a least a certain number of times in the dataset are kept. Index 0 is reserved for out-of-vocabulary words.\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"NU8DZ-kVPuBX"},"source":["### Importing libraries and downloading the data:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6fcO2sbrulEJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707140797582,"user_tz":-60,"elapsed":7842,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}},"outputId":"294297f0-5839-4a07-ccf9-ed838294b582"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import re\n","import json\n","import operator\n","from bs4 import BeautifulSoup\n","from collections import Counter\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from itertools import islice\n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from sklearn.metrics.pairwise import cosine_similarity as cosine\n","import nltk\n","nltk.download('stopwords')\n","\n","import warnings\n","import urllib\n","warnings.filterwarnings(\"ignore\")\n","\n","urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2151351&authkey=AHMClMM4ep3hNHw\", 'imdb_reviews.txt')\n","\n","path_read = './'\n","path_write =  './'"]},{"cell_type":"markdown","metadata":{"id":"z_9_vD4yCkGv"},"source":["\n","<b><h4><font color='blue'>\n","<hr style=\"border:10px solid blue\"> </hr>\n","Task 1: </b><br>\n","Fill the gaps in the next cellto perform the steps described above.\n","<hr style=\"border:10px solid blue\"> </hr>\n","</font></h4>"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"kaN3aNU_svtv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707141897319,"user_tz":-60,"elapsed":34426,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}},"outputId":"be621aa5-9e90-4ec6-e923-cea18ef3781e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 / 25000 reviews cleaned\n","5000 / 25000 reviews cleaned\n","10000 / 25000 reviews cleaned\n","15000 / 25000 reviews cleaned\n","20000 / 25000 reviews cleaned\n","counts saved to disk\n","vocab saved to disk\n","0 / 25000 reviews converted to ints\n","5000 / 25000 reviews converted to ints\n","10000 / 25000 reviews converted to ints\n","15000 / 25000 reviews converted to ints\n","20000 / 25000 reviews converted to ints\n","reviews saved to disk\n"]}],"source":["min_freq = 5 # retain the words appearing at least this number of times\n","oov_token = 0 # for out-of-vocabulary words\n","verbosity = 5\n","\n","# ========== read and clean reviews ==========\n","\n","with open(path_read + 'imdb_reviews.txt','r',encoding='utf-8') as file:\n","    reviews = file.readlines()\n","\n","tokenizer = TweetTokenizer()\n","\n","cleaned_reviews = []\n","\n","for counter,rev in enumerate(reviews):\n","    rev = rev.lower()\n","    temp = BeautifulSoup(rev,'lxml')\n","    text = temp.get_text() # remove HTML formatting\n","    text = re.sub(' +',' ',text) # strip extra white space\n","    text = text.strip() # strip leading and trailing white space\n","    tokens = tokenizer.tokenize(text) # tokenize\n","    cleaned_reviews.append(tokens)\n","    if counter % round(len(reviews)/verbosity) == 0:\n","        print(counter, '/', len(reviews), 'reviews cleaned')\n","\n","# ========== build vocab ==========\n","tokens = [token for sublist in cleaned_reviews for token in sublist]\n","counts = dict(Counter(tokens))\n","\n","### fill the gap (filter the dictionary 'counts' by retaining only the words that appear at least 'min_freq' times)\n","counts = counts = {word: count for word, count in counts.items() if count >= min_freq}\n","\n","\n","with open(path_write + 'counts.json', 'w') as file:\n","    json.dump(counts, file, sort_keys=True, indent=4)\n","\n","print('counts saved to disk')\n","\n","sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n","\n","# assign to each word an index based on its frequency in the corpus\n","# the most frequent word will get index equal to 1\n","# 0 is reserved for out-of-vocabulary words\n","word_to_index = dict([(my_tuple[0],idx) for idx,my_tuple in enumerate(sorted_counts,1)])\n","\n","# examples\n","word_to_index['the']\n","word_to_index[\"don't\"]\n","\n","with open(path_write + 'vocab.json', 'w') as file:\n","    json.dump(word_to_index, file, sort_keys=True, indent=4)\n","\n","print('vocab saved to disk')\n","\n","# ========== transform each review into a list of word indexes ==========\n","\n","\n","reviews_ints = []\n","\n","for i,rev in enumerate(cleaned_reviews):\n","    sublist = []\n","    ### fill the gaps (for the tokens that are not in 'word_to_index', use 'oov_token') ###\n","    for token in rev:\n","      if token in word_to_index:\n","        sublist.append(word_to_index[token])\n","      else :\n","        sublist.append(oov_token)\n","    reviews_ints.append(sublist)\n","\n","    if i % round(len(cleaned_reviews)/verbosity) == 0 :\n","      print(i, '/', len(reviews), 'reviews converted to ints' )\n","\n","\n","with open(path_write + 'doc_ints.txt', 'w') as file:\n","    for rev in reviews_ints:\n","        file.write(' '.join([str(elt) for elt in rev]) + '\\n')\n","\n","print('reviews saved to disk')"]},{"cell_type":"markdown","metadata":{"id":"AXaFsFkXAz4E"},"source":["<h3><b>4. Training example sampling</b></h2>\n","<p style=\"text-align: justify;\">\n","Skip-gram is trained on the artificial task of context prediction. The word vectors are learned as a side-effect of training (they are actually the parameters of the model). More precisely, pairs of target and context words (t, Ct ) are sampled by sliding a window over the corpus {0, 1, . . . , T }. For a given instantiation of the window, the word in the middle is the target word t, and the words surrounding it compose the set of positive examples Ct. In practice, the window size is not fixed but uniformly sampled in [1, max window size].\n","\n","<b><h4><font color='blue'>\n","<hr style=\"border:10px solid blue\"> </hr>\n","Task 2: </b><br>\n","Fill the first gaps in the sample_examples() function in the next code cell to perform the aforementioned steps.\n","<hr style=\"border:10px solid blue\"> </hr>\n","</font></h4>\n","</p>"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"jC_-OgQ0BOA9","executionInfo":{"status":"ok","timestamp":1707144198207,"user_tz":-60,"elapsed":324,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["def get_windows(seq,n):\n","    '''\n","    returns a sliding window (of width n) over data from the iterable\n","    taken from: https://stackoverflow.com/questions/6822725/rolling-or-sliding-window-iterator/6822773#6822773\n","    '''\n","    it = iter(seq)\n","    result = tuple(islice(it, n))\n","    if len(result) == n:\n","        yield result\n","    for elem in it:\n","        result = result[1:] + (elem,)\n","        yield result\n","\n","def sample_examples(docs,max_window_size,n_windows,neg_distr):\n","    '''generate target,context pairs and negative examples'''\n","    windows = []\n","    for i,doc in enumerate(docs):\n","        ### fill the gaps (get windows of size 'window_size' from the current document. Sample 'window_size' uniformly in {1,...,max_window_size}) ###\n","        window_size = int(np.random.choice(range(1, max_window_size+1), 1))\n","        windows.append(list(get_windows(doc, 2*window_size+1)))\n","\n","    windows = [elt for sublist in windows for elt in sublist] # flatten\n","    print(len(windows))\n","    random_idxs = np.random.choice(range(len(windows)),size=n_windows,replace=False)\n","    windows = [windows[idx] for idx in random_idxs]\n","\n","    ### fill the gap (sample n_negs*len(windows) negatives according to some probability distribution (negs_distr)###\n","    all_negs = list(np.random.choice(token_ints, size= n_negs*len(windows), p=neg_distr))\n","\n","    return windows,all_negs\n"]},{"cell_type":"markdown","metadata":{"id":"udVTueaeBTrg"},"source":["<h3><b>5. Objective</b></h2>\n","<p style=\"text-align: justify;\">\n","Skip-gram is then trained to assign high probabilities to the words in $C_t$, given t, i.e., to predict well the context of a given target word. This translates into the following log-likelihood:\n","$$argmax_{Î¸}\\sum^{T}_{t=0}\\sum_{câˆˆC_{t}}logp(c|t;\\theta)$$\n","The set of parameters of the model, Î¸, contains two matrices of word vectors, say $W_t$ and $W_c$, from which are drawn the vectors of the words when they are used as targets and contexts, respectively. If we denote by V the vocabulary (unique words), and by d the dimensionality of the word embedding space, we can assume that $W_t$ and Wc live respectively in $R^{|V|Ã—d}$ and $R^{dÃ—|V|}$. These two matrices are often referred to as input and output matrices, and it is common practice to use the input matrix, after training, as the final word embeddings.<br>\n","<b>Softmax vs. negative sampling</b>: The predictions p(c|t; Î¸) âˆ€c âˆˆ $C_t$ are given by looking at the entries of the vector in $R^{1Ã—|V|}$ obtained by passing the vector $w_t$ âˆˆ $R^d$ of the target word to a simple linear layer parameterized by $W_c$. That is, by multiplying $w_t$ with matrix $W_c$[This operation is fully linear: no nonlinear activation function is applied (there is no hidden layer). Because of this, skip- gram cannot really be considered to be a deep learning model. The term shallow neural network, or log-linear model, is more appropriate. The same holds for CBOW]. Further, to ensure that legitimate probabilities are obtained, we normalize with a softmax:\n","\n","$$ p(c|t;\\theta)=\\frac{e^{w_c.w_t}}{\\sum_{vâˆˆV}e^{w_v.w_t}}$$\n","However, this approach is expensive because of the large size of the vocabulary V ($10^5$ or $10^6$).\n","<b>Negative sampling trick:</b> learning to discriminate. Rather than trying to assign a probability to each single word in the vocabulary, another approach consists in performing independent discrimination tasks. In this scenario, we teach the model to distinguish between words from the true context $C_{t}^{+}$ of a given target word and negative examples $C_{t}^{-}$, i.e., words that are sampled at random from the vocabulary:\n","$$argmax_{\\theta}âˆ‘_{t=1}^{T}(âˆ‘_{c âˆˆC_{t}^{+}}logp(c|t;Î¸)+âˆ‘_{c âˆˆC_{t}^{-}$}log(1-p(c|t;Î¸)))$$\n","We now just have independent binary classification tasks to perform. This comes with a major computational advantage: instead of multiplying $w_t$ with the full matrix $W_c$ âˆˆ $R^{dÃ—|V|}$ (where |V| can get very\n","large), we now just have to compute a few dot products between $w_t$ and the words in $C_{t}^{+} âˆª C_{t}^{âˆ’}$. Assuming that the labels (pos/neg) are indicated by Â±1, the individual predictions can be obtained with the sigmoid function Ïƒ(x) = $\\frac{1}{1-e^{-w_c.w_t}}$ Plugging this in the previous equation, and using the fact that 1 âˆ’ Ïƒ(x) = Ïƒ(âˆ’x), we obtain the loss:\n","$$argmin_{\\theta}âˆ‘_{t=1}^{T}(âˆ‘_{c âˆˆC_{t}^{+}}log(1+e^{-w_c.w_t})+âˆ‘_{c âˆˆC_{t}^{-}$}log(1+e^{w_c.w_t}))$$\n","Note that in practice, the negative examples are not sampled uniformly but proportionally to their dampened frequency (square root of their frequency, to be exact)[<a href='https://arxiv.org/abs/1607.04606'>Bojanowski et al., 2017</a>]\n","\n","<b>Note that in our code, both $W_c$ and $W_t$ âˆˆ $R^{|V|xd}$  so the word embedding of the target and context can be accessed by $W_t[target,]$ and $W_c[context,]$</b>\n","\n","<b><h4><font color='blue'>\n","<hr style=\"border:10px solid blue\"> </hr>\n","Task 3: </b><br>\n","Fill the second gap in the sample_examples() function in the previous code cell to implement negative exampkle sampling.\n","<hr style=\"border:10px solid blue\"> </hr>\n","</font></h4>\n","\n","<b><h4><font color='blue'>\n","<hr style=\"border:10px solid blue\"> </hr>\n","Task 4: </b><br>\n","Fill the gaps in the compute_loss() function of the next cell to compute the loss for one training example.\n","<hr style=\"border:10px solid blue\"> </hr>\n","</font></h4>\n","</p>"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"7p4lQcbFHjip","executionInfo":{"status":"ok","timestamp":1707148758712,"user_tz":-60,"elapsed":306,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["def compute_dot_products(pos,negs,target):\n","  # pos:list contaning the indices of the positive context\n","  # negs:list contaning the indices of the negative context\n","  # target: the index of the target word\n","    prods = Wc[pos+negs,] @ Wt[target,] # (n_pos+n_negs,d) X (d,) -> (n_pos+n_negs,)\n","    return prods\n","\n","def compute_loss(prodpos,prodnegs):\n","  # prodpos: wc.wt for positive context\n","  # prodnegs: wc.wt for negative context\n","  # wt: word embedding for the target word\n","  # wc: word embedding for the context\n","    '''prodpos and prodnegs are numpy vectors containing the dot products of the context word vectors with the target word vector'''\n","    ### fill the gaps ###\n","    term_pos = np.log(1 + np.exp(-prodpos)) # positive context\n","    term_negs = np.log(1 + np.exp(prodnegs)) # negative context\n","    return np.sum(term_pos) + np.sum(term_negs)"]},{"cell_type":"markdown","metadata":{"id":"yHf1bwk6GGn0"},"source":["<h3><b>6. Computing the gradient</b></h2>\n","<p style=\"text-align: justify;\">\n","Skip-gram is trained with SGD. We thus need to compute the gradient to perform updates at each training iteration. Let us consider our loss for a given training example (t, $C_{t}^{+} , C_{t}^{âˆ’}$ ):\n","$$L(t, C_{t}^{+} , C_{t}^{âˆ’}) = \\sum_{c \\in \\mathcal{C}_t^+ } \\log \\big(1+e^{-w_c \\cdot w_t}\\big) + \\sum_{c \\in \\mathcal{C}_t^-} \\log \\big(1+e^{w_c \\cdot w_t}\\big)$$\n","$L$ is a function of the target word $t$ and of all context words, i.e., all words in $\\mathcal{C}_t^+ \\cup \\mathcal{C}_t^-$.\n","</p>\n","\n","\n","<b><h4><font color='red'>\n","<hr style=\"border:10px solid red\"> </hr>\n","Question 1 (3 points): </b><br>\n","Compute the partial derivatives of the loss w.r.t one positive example and one negative example, $\\frac{\\partial L}{\\partial w_{c^+}}$ and $\\frac{\\partial L}{\\partial w_{c^\\text{-}}}$.\n","<hr style=\"border:10px solid red\"> </hr>\n","</font></h4>\n"]},{"cell_type":"markdown","metadata":{"id":"AfnpxmskGtfI"},"source":["<b><h4><font color='green'>\n","<hr style=\"border:10px solid green\"> </hr>\n","Answer 1: </b><br>\n","Your answer here.\n","<hr style=\"border:10px solid green\"> </hr>\n","</font></h4>"]},{"cell_type":"markdown","source":["$\\frac{\\partial L}{\\partial w_{c^+}}$ = $\\frac {- w_{t}}{1+ e^{w_{c^+} . w_{t}} }$\n","\n","$\\frac{\\partial L}{\\partial w_{c^-}}$ = $\\frac { w_{t}}{1+ e^{-w_{c^-} . w_{t}} }$"],"metadata":{"id":"ol0T30dc4_-6"}},{"cell_type":"markdown","metadata":{"id":"l4Y3JU-8IypR"},"source":["<b><h4><font color='red'>\n","<hr style=\"border:10px solid red\"> </hr>\n","Question 2 (3 points): </b><br>\n","Compute the partial derivative of the loss w.r.t. the target word, $\\frac{\\partial L}{\\partial w_t}$\n","<hr style=\"border:10px solid red\"> </hr>\n","</font></h4>\n"]},{"cell_type":"markdown","metadata":{"id":"2-19jtgOI_2e"},"source":["<b><h4><font color='green'>\n","<hr style=\"border:10px solid green\"> </hr>\n","Answer 2: </b><br>\n","Your answer here.\n","<hr style=\"border:10px solid green\"> </hr>\n","</font></h4>"]},{"cell_type":"markdown","source":["$\\frac{\\partial L}{\\partial w_{t}}$ = $\\frac {- w_{c}}{1+ e^{w_{c} . w_{t}} }$\n","\n","$\\frac{\\partial L}{\\partial w_{c}}$ = $\\frac { w_{t}}{1+ e^{-w_{c} . w_{t}} }$.\n","\n","\n","\n","So :\n","\n","\n","$$\\frac{\\partial L}{\\partial w_{t}} = - \\sum_{c \\in \\mathcal{C}_t^+ } \\frac { w_{c}}{1+ e^{w_{c} . w_{t}} } + \\sum_{c \\in \\mathcal{C}_t^-} \\frac { w_{c}}{1+ e^{-w_{c} . w_{t}} } $$"],"metadata":{"id":"meim06Ej7qh7"}},{"cell_type":"markdown","metadata":{"id":"EHjDW5uIJGqo"},"source":["<b><h4><font color='blue'>\n","<hr style=\"border:10px solid blue\"> </hr>\n","Task 5: </b><br>\n","Fill the gaps in the compute_gradients() function in the next cell to compute the partial derivatives for one training example.\n","<hr style=\"border:10px solid blue\"> </hr>\n","</font></h4>"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"cGzuj8GqIw2h","executionInfo":{"status":"ok","timestamp":1707149947760,"user_tz":-60,"elapsed":329,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}}},"outputs":[],"source":["def compute_gradients(pos,negs,target,prodpos,prodnegs, Wt, Wc):\n","    # Wt is the targe embedding matrix of dimension (#vocab, d) where d is the embedding dimension\n","    # Wc is the context embedding matrix of dimension (#vocab, d) where d is the embedding dimension\n","    factors_pos = 1/(np.exp(prodpos)+1).reshape(-1,1) # vector of shape (n_pos,1)\n","    factors_negs = 1/(np.exp(-prodnegs)+1).reshape(-1,1) # vector of shape (n_negs,1)\n","\n","    ### fill the gaps using factors_pos, factors_negs, Wt and Wc###\n","    # positive/negative examples\n","    partials_pos = factors_pos @ -Wt[target,].reshape(1,-1) # (n_pos,1) X (1,d) -> (n_pos,d), question 1 results\n","    partials_negs = factors_negs @ -Wt[target,].reshape(1,-1) #question 1 results\n","\n","    # target word\n","    term_pos = -Wc[pos,] * factors_pos\n","    term_negs = -Wc[negs,] * factors_negs\n","    partial_target = np.sum(term_pos,axis=0) + np.sum(term_negs,axis=0)\n","\n","    '''\n","    # hint: NumPy broadcasting example\n","    import numpy as np\n","    A = np.array([[1,2,3],[1,2,3],[1,2,3]])\n","    a = np.array([1,10,100]).reshape(-1,1)\n","    A * a\n","    '''\n","\n","    return partials_pos,partials_negs,partial_target\n","\n","def my_cos_similarity(word1,word2):\n","    sim = cosine(Wt[vocab[word1],].reshape(1,-1),Wt[vocab[word2],].reshape(1,-1))\n","    return round(float(sim),4)"]},{"cell_type":"markdown","metadata":{"id":"e_1AxQQFJSkX"},"source":["<h3><b>7. Performing updates</b></h2>\n","<p style=\"text-align: justify;\">\n","The model is trained via stochastic gradient descent.\n","The representation of each word is updated based on its corresponding partial derivative.\n","For instance, for a positive word at iteration $n$, we perform the following update:\n","\n","\\begin{equation}\n","w_{c^+_{~n+1}} = w_{c^+_{~n}} - \\gamma_n \\frac{\\partial L}{\\partial w_{c^+_{~n}}}\n","\\end{equation}\n","\n","\\noindent where $\\gamma_n$ is the learning rate at iteration $n$.\n","Note that we use $\\gamma_n$ and not $\\gamma$ because the learning rate is not fixed, but annealed during training.\n","The schedule we use is $\\gamma_n =  \\frac{\\gamma_0}{1+\\text{decay}\\times n}$, where decay $=10^{-6}$.\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"foIg15WqJiEk"},"source":["<b><h4><font color='blue'>\n","<hr style=\"border:10px solid blue\"> </hr>\n","Task 6: </b><br>\n","Fill the gaps in the nested for loop of the next cell to perform the updates.\n","<hr style=\"border:10px solid blue\"> </hr>\n","</font></h4>"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"g4BIqh0KJltG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707151444450,"user_tz":-60,"elapsed":1492528,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}},"outputId":"06a5ab08-110c-4250-b853-7aca0d56c6bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["6399752\n","training examples sampled\n","epoch 1 / 15  finished, loss:  inf lr: 0.012500006250003127\n","6399832\n","training examples sampled\n","epoch 2 / 15  finished, loss:  inf lr: 0.008333336111112038\n","6399594\n","training examples sampled\n","epoch 3 / 15  finished, loss:  inf lr: 0.006250001562500392\n","6399764\n","training examples sampled\n","epoch 4 / 15  finished, loss:  inf lr: 0.005000001000000201\n","6399998\n","training examples sampled\n","epoch 5 / 15  finished, loss:  inf lr: 0.0041666673611112275\n","6399358\n","training examples sampled\n","epoch 6 / 15  finished, loss:  inf lr: 0.0035714290816327263\n","6398796\n","training examples sampled\n","epoch 7 / 15  finished, loss:  inf lr: 0.003125000390625049\n","6399590\n","training examples sampled\n","epoch 8 / 15  finished, loss:  inf lr: 0.002777778086419788\n","6399490\n","training examples sampled\n","epoch 9 / 15  finished, loss:  inf lr: 0.0025000002500000255\n","6399444\n","training examples sampled\n","epoch 10 / 15  finished, loss:  inf lr: 0.002272727479338862\n","6398984\n","training examples sampled\n","epoch 11 / 15  finished, loss:  inf lr: 0.002083333506944459\n","6399306\n","training examples sampled\n","epoch 12 / 15  finished, loss:  inf lr: 0.0019230770710059288\n","6398774\n","training examples sampled\n","epoch 13 / 15  finished, loss:  inf lr: 0.0017857144132653155\n","6399254\n","training examples sampled\n","epoch 14 / 15  finished, loss:  inf lr: 0.0016666667777777853\n","6399470\n","training examples sampled\n","epoch 15 / 15  finished, loss:  inf lr: 0.0015625000976562562\n","word vectors saved to disk\n"]}],"source":["# = = = = = = = = = = = = = = = = = = = = =\n","stpwds = set(stopwords.words('english'))\n","\n","max_window_size = 5 # extends on both sides of the target word\n","n_windows = int(1e6) # number of windows to sample at each epoch\n","n_negs = 5 # number of negative examples to sample for each positive\n","d = 30 # dimension of the embedding space\n","n_epochs = 15\n","lr_0 = 0.025\n","decay = 1e-6\n","\n","with open(path_read + 'doc_ints.txt', 'r') as file:\n","    docs = file.read().splitlines()\n","\n","docs = [[int(eltt) for eltt in elt.split()] for elt in docs]\n","\n","with open(path_read + 'vocab.json', 'r') as file:\n","    vocab = json.load(file)\n","\n","vocab_inv = {v:k for k,v in vocab.items()}\n","\n","with open(path_read + 'counts.json', 'r') as file:\n","    counts = json.load(file)\n","\n","token_ints = range(1,len(vocab)+1) # list of tokens from which we sample all_negs\n","neg_distr = [counts[vocab_inv[elt]] for elt in token_ints]\n","neg_distr = np.sqrt(neg_distr)\n","neg_distr = neg_distr/sum(neg_distr) # normalize\n","\n","# ========== train model ==========\n","\n","total_its = 0\n","\n","Wt = np.random.normal(size=(len(vocab)+1,d)) # + 1 is for the OOV token\n","Wc = np.random.normal(size=(len(vocab)+1,d))\n","\n","for epoch in range(n_epochs):\n","\n","    windows,all_negs = sample_examples(docs,max_window_size,n_windows,neg_distr)\n","    print('training examples sampled')\n","\n","    np.random.shuffle(windows)\n","\n","    total_loss = 0\n","\n","\n","    for i,w in enumerate(windows):\n","\n","        target = w[int(len(w)/2)] # elt at the center\n","        pos = list(w)\n","        del pos[int(len(w)/2)] # all elts but the center one\n","\n","        negs = all_negs[n_negs*i:n_negs*i+n_negs]\n","\n","        prods = compute_dot_products(pos,negs,target)\n","        prodpos = prods[0:len(pos),]\n","        prodnegs = prods[len(pos):(len(pos)+len(negs)),]\n","\n","        partials_pos, partials_negs, partial_target = compute_gradients(pos,negs,target,prodpos,prodnegs, Wt, Wc)\n","\n","        lr = lr_0 * 1/(1+decay*total_its)\n","        total_its += 1\n","\n","        ### fill the gaps (perform the updates) ###\n","        Wt[target,] -= lr * partial_target\n","        Wc[pos,] -= lr * partials_pos\n","        Wc[negs,] -= lr * partials_negs\n","\n","        total_loss += compute_loss(prodpos,prodnegs)\n","    print('epoch', epoch+1 , '/', n_epochs ,' finished, loss: ', total_loss/(i+1), 'lr:', lr)\n","\n","\n","np.save(path_write + 'input_vecs',Wt,allow_pickle=False) # pickle disabled for portability reasons\n","np.save(path_write + 'output_vecs',Wc,allow_pickle=False)\n","\n","print('word vectors saved to disk')"]},{"cell_type":"markdown","metadata":{"id":"RhD5jVxlJ2sg"},"source":["<b><h4><font color='blue'>\n","<hr style=\"border:10px solid blue\"> </hr>\n","Task 7: </b><br>\n","Train the model for several epochs. Monitor your loss to verify that the model learns. Then, fill the gaps at the last code cell to compute some similarities and visualize the learned embeddings.\n","<hr style=\"border:10px solid blue\"> </hr>\n","</font></h4>"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"kqb3B1PnMLnt","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"error","timestamp":1707151598255,"user_tz":-60,"elapsed":355,"user":{"displayName":"Ange Gounadon","userId":"05431015772885463032"}},"outputId":"c5418a47-dd89-4f86-e020-b4e01b58a5e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["movie vs. film: 0.0\n","movie vs. banana: 0.0\n","film vs. banana: 0.0\n"]},{"output_type":"error","ename":"ValueError","evalue":"cannot reshape array of size 5024 into shape (315,16)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-5c760183dbe8>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmy_pca_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWt_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmy_tsne_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_tsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_pca_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mdistances_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"distance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors_graph\u001b[0;34m(self, X, n_neighbors, mode)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"distance\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0mA_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0mA_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0mdup_gr_nbrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0msample_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdup_gr_nbrs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 5024 into shape (315,16)"]}],"source":["Wt = np.load(path_write + 'input_vecs.npy')\n","Wc = np.load(path_write + 'output_vecs.npy')\n","# ========== sanity checks ==========\n","# = = some similarities = =\n","### fill the gaps (compute the cosine similarity between some (un)related words, like movie/film/banana ###\n","# Compute cosine similarities\n","similarities = my_cos_similarity('movie', 'film')\n","print('movie vs. film:', similarities)\n","similarities_ = my_cos_similarity('movie', 'banana')\n","print('movie vs. banana:', similarities_)\n","_similarities = my_cos_similarity('film', 'banana')\n","print('film vs. banana:', _similarities)\n","\n","\n","# = = visualization of most frequent tokens = =\n","\n","n_plot = 500\n","mft = [vocab_inv[elt] for elt in range(1,n_plot+1)]\n","\n","# exclude stopwords and punctuation\n","keep_idxs = [idx for idx,elt in enumerate(mft) if len(elt)>3 and elt not in stpwds]\n","mft = [mft[idx] for idx in keep_idxs]\n","keep_ints = [list(range(1,n_plot+1))[idx] for idx in keep_idxs]\n","Wt_freq = Wt[keep_ints,]\n","\n","### fill the gaps (perfom PCA (10D) followed by t-SNE (2D). For t-SNE, you can use a perplexity of 5.) ###\n","### for t-SNE, see https://lvdmaaten.github.io/tsne/#faq ###\n","my_pca = PCA(n_components=10)\n","my_tsne = TSNE(n_components=2, perplexity=5)\n","\n","my_pca_fit = my_pca.fit_transform(Wt_freq)\n","my_tsne_fit = my_tsne.fit_transform(my_pca_fit)\n","\n","fig, ax = plt.subplots()\n","ax.scatter(my_tsne_fit[:, 0], my_tsne_fit[:, 1],s=3) ### fill the gap ###\n","\n","for x,y,token in zip(my_tsne_fit[:, 0], my_tsne_fit[:, 1],mft): ### fill the gap ###\n","    ax.annotate(token, xy=(x,y), size=8)\n","\n","fig.suptitle('t-SNE visualization of word embeddings',fontsize=20)\n","fig.set_size_inches(11,7)\n","fig.savefig(path_write + 'word_embeddings.pdf',dpi=300)\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{"id":"W9TivlrnKIbs"},"source":["<b><h4><font color='red'>\n","<hr style=\"border:10px solid red\"> </hr>\n","Question 3 (4 points): </b><br>\n","Observe and interpret your similarity values and your plot. What can you say about the embedding space?\n","<hr style=\"border:10px solid red\"> </hr>\n","</font></h4>\n"]},{"cell_type":"markdown","metadata":{"id":"sL0TzRhuKkfz"},"source":["<b><h4><font color='green'>\n","<hr style=\"border:10px solid green\"> </hr>\n","Answer 3: </b><br>\n","Your answer here.\n","<hr style=\"border:10px solid green\"> </hr>\n","</font></h4>"]},{"cell_type":"markdown","metadata":{"id":"qa8d73a-KSxJ"},"source":["<b><h4><font color='red'>\n","<hr style=\"border:10px solid red\"> </hr>\n","Question 4 (10 points): </b><br>\n","What changes should we make to our pipeline (preprocessing and training) to learn document vectors jointly with the word vectors? Base your answer on [<a href='https://arxiv.org/abs/1405.4053'>Le et Mikolov, 2014</a>]\n","<hr style=\"border:10px solid red\"> </hr>\n","</font></h4>\n"]},{"cell_type":"markdown","metadata":{"id":"a5j-PvYaKk2p"},"source":["<b><h4><font color='green'>\n","<hr style=\"border:10px solid green\"> </hr>\n","Answer 4: </b><br>\n","Your answer here.\n","<hr style=\"border:10px solid green\"> </hr>\n","</font></h4>"]}],"metadata":{"colab":{"provenance":[{"file_id":"1xFKwjFOh8b1W-opFfsUFxtfhRH9dpUZo","timestamp":1707140683235},{"file_id":"1daoFvMTE9P5CEkSyK9Jrt-K6ez2Yyh3Z","timestamp":1707083956425}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.10 (main, Jan 15 2022, 11:40:53) \n[Clang 13.0.0 (clang-1300.0.29.3)]"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}